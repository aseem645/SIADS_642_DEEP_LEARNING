{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff8a6b6a84b18fbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Recurrent Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "We now move from image recognition to natural language processing. For this assignment, we will work with a common sentiment analysis task using the IMDB dataset. This set consists of review-label pairs, where the task is to predict whether the text of the given movie review is positive or negative, a binary classification.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "You will be comparing four different recurrent neural network architectures: a standard RNN, a Gated Recurrent Unit (GRU), a standard Long Short-Term Memory (LSTM), and a bidirectional LSTM. \n",
    "\n",
    "Note that a GRU/LSTM cell _is_ an RNN cell, but we will refer to an RNN in the code and questions below as the most basic RNN.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are four short answer questions testing your understanding of this neural network architecture. As before, some questions will require you to experiment with model hyperparameters. Additionally, you will need to produce and analyze a graph.\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a .PDF file by choosing File - Download as - pdf (.pdf). You will be submitting this .pdf to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week3'\n",
    "reviewVocabVectors = pickle.load(open(root_dir + '/reviewVocabVectors', 'rb'))\n",
    "trainIterator = pickle.load(open(root_dir + '/trainIterator', 'rb'))\n",
    "testIterator = pickle.load(open(root_dir + '/testIterator', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 10\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 5\n",
    "vocabSize = 20002\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        \n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        \n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths)\n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput)\n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN = MyRNN(model='RNN')\n",
    "GRU = MyRNN(model='GRU') # Construct a GRU model, as above\n",
    "LSTM = MyRNN(model='LSTM') # Construct a LSTM model, as above\n",
    "biLSTM = MyRNN(model='BiLSTM') # Construct a BiLSTM model, as above\n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(reviewVocabVectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Epoch: 1, Train Loss: 0.7023946528544511\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6915372325026471\n",
      "Model: RNN, Epoch: 3, Train Loss: 0.6850547603024241\n",
      "Model: RNN, Epoch: 4, Train Loss: 0.6643173133625704\n",
      "Model: RNN, Epoch: 5, Train Loss: 0.6291575803781104\n",
      "\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.7004434310871622\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.6853625314010073\n",
      "Model: GRU, Epoch: 3, Train Loss: 0.6251700029653662\n",
      "Model: GRU, Epoch: 4, Train Loss: 0.4790666857186486\n",
      "Model: GRU, Epoch: 5, Train Loss: 0.387260852453044\n",
      "\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.694226009461581\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.6681870629110604\n",
      "Model: LSTM, Epoch: 3, Train Loss: 0.6659568180056179\n",
      "Model: LSTM, Epoch: 4, Train Loss: 0.560460474027697\n",
      "Model: LSTM, Epoch: 5, Train Loss: 0.5663308347277629\n",
      "\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.6936177038170798\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.684041075541845\n",
      "Model: BiLSTM, Epoch: 3, Train Loss: 0.6048695712595644\n",
      "Model: BiLSTM, Epoch: 4, Train Loss: 0.4761252319416427\n",
      "Model: BiLSTM, Epoch: 5, Train Loss: 0.38348551715731316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / len(trainIterator)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Validation Accuracy: 72.0428388746803%\n",
      "Model: GRU, Validation Accuracy: 81.72234654731459%\n",
      "Model: LSTM, Validation Accuracy: 79.09846547314578%\n",
      "Model: BiLSTM, Validation Accuracy: 82.27621483375958%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        accuracy = 0.0\n",
    "        for batch in testIterator:\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = batchAccuracy(predictions, batch[1])\n",
    "            accuracy += acc\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / len(testIterator) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f36ec050380d95b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a084a6888e27954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Coding (50 points)\n",
    "\n",
    "First, run the code given above to assess accuracy of the default RNN model. Note that it is normal for box 3 to take a significant amount of time to run as it is loading a large text dataset.\n",
    "\n",
    "Next, you will need to construct three other model types (GRU, LSTM, BiLSTM) for comparison purposes. Follow the comments in box 4 to initialize the three other model types then rerun the code with all models enabled.\n",
    "\n",
    "Finally, compare the accuracies of all four models (the accuracy of the default RNN should not change from the initial run). Explain your results. In doing so, overview the advantages of the best performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-503b047d28162c58",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "RNN MODEL: The base level RNN model has a calculated validation accuracy of 72.04 on the test set. \n",
    "\n",
    "GRU MODEL: The GRU model has a calculated validation accuracy of 81.72 on the test set.\n",
    "\n",
    "LSTM MODEL: The LSTM model has a calculated validation accuracy of 79.09 on the test set.\n",
    "\n",
    "BiLSTM MODEL: The ViLSTM had the best validation accuracy on the test set, at 82.28.\n",
    "\n",
    "The fact that the RNN MODEL had the lowest accuracy is not entirely surprising. The RNN model is much more susceptible to the vanishing/exploding gradient problem as compared to the other models since backpropogation through time necessitates lots of multiplication operations. If the gradient shrinks exponentially, then that means the weights of the initial layers of the network will not be updated effectively on the backwards pass. On the flipside, if the gradient explodes excessively, the model weights will be updated by extremely large increments, which may cause the model to converge on a suboptimal solution too early and makes it difficult for the model to learn on long input sequences. In regards to other shortcomings, vanila RNNs are typically slow to train because RNN steps are difficult to parralelize. Since each step is dependent on the previous one, one cannot parralelize the computations.\n",
    "\n",
    "Of the other three model types - LSTM, BiLSTM, and GRU, LSTM performs the worst comparatively, GRU the second best, and BiLSTM the best, though the accuracies of all three are quite close to one another. I somewhat expected for the LSTM to at least have better performance than the RNN, because the LSTM is not nearly as susceptible to the vanishing/exploding gradient problem as vanilla RNNs. This is because the forget gate, input gate, and output gate all work in tandem to mitigate this issue by controlling the value of the gradient as it passes through, thereby preventing it from getting excessively small or excessively large. The GRU happens to perform better than the LSTM. It has fewer parameters than the former, and only has two gates to control the flow of information - the reset gate and the update gate - thereby potentially reducing some redundancies in functionality of the gates that may have occured in the LSTM model, and also took less time to train than the LSTM model and RNN model. \n",
    "\n",
    "The BiLSTM model, however, slightly edges out the GRU model and performs the best of the bunch. BiLSTMs still have all the advantages that LSTMs have over vanilla RNNs(controlling the gradient size and accessing long range sequences), with the added benefit that, for each point in a given sequence, the bidirectional neural net knows about points not only before the point but also after it. In the context of this movie review problem, this gives the BiLSTM an edge in the sense that the model can gain information by also taking into account what hasn't been said yet about the movie in question, in addition to what has already been said, providing it with potentially valuable additional context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3920676c328b92ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: LSTM Gates (30 points)\n",
    "\n",
    "LSTMs improve upon the naive RNN architecture by adding a series of gates instead of a simple matrix-vector computation. Name the gates and explain each of their functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2bb845467aba5dd6",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Three different gates regulate the flow of information in an LSTM - the forget gate, the input gate, and the output gate. \n",
    "\n",
    "Forget Gate: This gate decides what information that passes through it should be kept, and what should be thrown away. The information that flows to it comes from the previous hidden state and the current input. This info is fed into a sigmoid activation function within the gate, and if the value is close to zero, it implies that the info should be forgotten, and if it is close to 1, it implies that the info should be kept.\n",
    "\n",
    "Input Gate: The input gate is used to update the cell state. The info from the previous hidden state and the current input, like the forget gate, is passed into a sigmoid function within the gate. If the resulting value is close to 0, that value will not be updated, but if it is close to 1, then it will. However, unlike the forget gate, there is also a tanh activation function within the input gate, and the hidden state/current input info is passed into that function as well. Then, the tanh output is multiplied with the sigmoid output, resulting in either a zero or nonzero value. If the value is nonzero, then that means the info is important.\n",
    "\n",
    "\n",
    "Output Gate: The output gate is the final gate, and it ultimately decides what the next hidden state should be. Firstly, the previous hidden state and the current input is, like the previous two gates, passed into a sigmoid function. Then, the newly modified cell state from the input gate is passed to a tanh function(also in the output gate). The tanh output is multiplied with the sigmoid output to determine what info the hidden state should carry(using zero/nonzero as usual). Then, the hidden state and the modified cell state is output and passed along to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab00430a80313063",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Applications (20 points)\n",
    "\n",
    "LSTMs are used across many different fields, from music generation to sentiment classification to text generation. Where could they be useful in your life, whether at home, for your family, or in the workplace? Give a specific problem or application for an LSTM model that was not covered in the course slides. Then, with your application in mind, specifically identify the input to your model, the output from your model, and an applicable loss function. \n",
    "\n",
    "(As an extension, try implementing your LSTM on your own using the code framework given in this homework!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51a908d0d4f3f225",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "LSTM Many inputs to one output(Stock Market): In my daily life, LSTMs could certainly play a role in my investment finances, specifically when it comes to investing in individual stocks, where it can be difficult and risky for one to solely rely on their own intuition. In the realm of stock market investing, LSTMs could be particularly advantageous, since they are not really inhibited by sequence length(whereas RNNs are inhibited by long sequences) and can store long-running historical information about a stock's price. This is important, because the historical price of a stock is crucial to know when attempting to make any kind of prediction regarding its future price. For this model, the input will be many historical data points pertaining to stock price, and it will output a single stock price prediction for the future. A linear activation function might work well in this case since we are trying to predict a numerical value.\n",
    "\n",
    "\n",
    "LSTM Many inputs to one output(Rhythm Generation): LSTMs would also be beneficial in my daily life in regards to music. As an avid drummer and percussionist, a robust LSTM could could create novel rhythmic compositions for me by feeding it example compositions during training. For this particular use case, an LSTM would be superior to an RNN because, in most real-life rhytmic compositions, the beats often do not follow each other rapidly one after the other. In many cases, there are significant intervals of time where there is no sound at all between beats, and an RNN might have difficulty accessing rhythmic data in a sequence where there are long intervals of pauses(in other words, no sound). The input to this model would be a sequence of rhythms, and the output would be a single note indicating the next rhythm.  RelU might be a good choice of activation function here since the goal would be to make the model robust to sparse inputs(cases where there are long pauses).\n",
    "\n",
    "\n",
    "LSTM Many inputs to one output(Feedback Classification): Although this doesn't have as much bearing in my current line of work as a software developer, in my previous role as a Quality Data Analyst, utilizing LSTMs for the purpose of feedback classification would have been a big asset. I dealt primarily with historical time series data of vehicle reviews written and submitted online by customers, and in my role, I had to manually read the aforementioned reviews and classify them into \"good\" and \"bad\" buckets. A robust LSTM would have been able to take each review as input, classify it as either good or bad, and improve its accuracy with time. In this particular case, a BiLSTM may be even better, since it would be able to take valuable context from the distant \"past\" for each particular review, as well as the \"future\", due to its ability to retain long sequences both preceding any given time step as well as after any given time step. The input to this model would be many reviews, and the output would be a single classification(good or bad review). Since we are predicting a binary outcome, sigmoid might be an appropriate activation function here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
