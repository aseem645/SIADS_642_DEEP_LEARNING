{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9caf0b25af95e666",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 4: Adversarial Models (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Finally, we will review adversarial machine learning models, a powerful paradigm that encompasses multiple classes of predictive models. These models can be used for anything from image generation to data augmentation to text translation.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "An adversarial model consists of two pieces: a generator and a discriminator. The generator produces some output using a given noise vector (for example, an artificial image). The goal of the discriminator is then to differentiate real images from generated images.\n",
    "\n",
    "The discriminator can be trained directly using ground-truth data, and the generator can then be trained in a feedback loop with the discriminator. In this way, the generator aims to fool the discriminator, and the discriminator aims to become more and more discerning.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Complete the questions below. Note that while most of this assignment refers to GANs (Generative Adversarial Networks), the first question speaks to generative models as a broader model architecture rather than GANs specifically.\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a .PDF file by choosing File - Download as - pdf (.pdf). You will be submitting this .pdf to your instructor for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b3da469e852c753",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4caca627cfcf41a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Generative Models vs Discriminative Models (20 points)\n",
    "\n",
    "What are the key differences between a generative model and a discriminative model from a statistical point of view? Explain them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7c8a18d9e9e06243",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "GANs are a neural network architecture that, in so many words, pits two neural networks against each other. One of them, the discriminator, is primarily responsible for mapping labels to inputs - in other words, it's a classifier. From a satistical notation standpoint, the discriminator captures the conditional probability P(Y|X), where X is the input and Y is the label, and outputs a probability between 0 and 1. In the context of image detection, the labels assigned might be \"fake\" vs \"real\", for instance. The generator, on the other hand, takes in N-dimensional uniform random variables(noise) and then generates a fake output. In the context of image detection, the generator would generate fake images. From a statistical notation standpoint, the generator captures the probability P(X), where X is the input.\n",
    "\n",
    "\n",
    "In regards to the purposes of each network, they are seemingly at odds with one another. The generator tries to maximize the classification error of the model, while the descriminator tries to minimize it, and both are isolated and trained seperately to optimize these goals. When training the discriminator, the focus is on minimizing the discriminator loss through backpropogation, which penalizes the discriminator for classifying real images as fake or vice versa. When training the generator, the focus is on minimizing the generator loss through backpropogation, which penalizes the generator for unsuccessfully fooling the discriminator(the discriminator successfully classifies a fake image as fake).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a2db2f4d5240ece",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: GAN Trade-offs (20 points)\n",
    "\n",
    "What are the main advantages and disadvantages of GANs over standard machine learning models? Explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a2a6cd11310b3e42",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "While GANs have the potential of being very good when put to use in the right contexts, that accuracy does not come easily, as they are very data hungry(require lots of training data) and can take a copious amount of time to train on commodity hardware like a normal laptop computer. Furthermore, a GAN is most effective when it is at equilibrium(when the generator and discriminator are roughly equally skilled), which is a very delicate balance and difficult to maintain. If the descriminator becomes incredibly good at recognizing fakes, the generative network will get stuck, whereas if the generator is too good, it will consistently be able to bypass the discriminator's validation checks and lead it to output false negatives.\n",
    "\n",
    "Furthermore, GANs are susceptible to a more malicious drawback known as an adversarial attack. In the context of image detection, this implies modifying the pixels or input features in such a way that the misclassification loss function is maximized, leading to labeling errors. In other words, adding a very small amount of noise to the input image, leading to classification errors and ultimately poor performance in the real world. Additionally, GANs are difficult to validate, and require expert knowledge to design and fine tune. Since the GAN does not output an accuracy score, it is necessary for a human eye to validate the accuracy of the labels it is assigning to the inputs.\n",
    "\n",
    "In regards to the advantages of GANs, one might be the fact that they are a type of unsupervised learning model, which may be advantageous in the sense that labeling data, especially datasets with very many observations, can take a copious amount of time to label. Another advantage of GANs is that they can oftentimes learn very complex and messy distributions of data, making them adaptable to many real world machine learning problems. Thirdly, GANs are able to generate realistic example inputs, potentially circumventing the need to gather a certain amount of example inputs manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fdaa5956fa3aa9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Adversarial Attacks (20 points)\n",
    "\n",
    "What is an adversarial attack on a machine learning model? Explain how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1834d95dca8b9d81",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In the context of machine learning, and more specifically image detection, an advesarial attack would constitute a deliberate injection of almost imperceptible noise into the model inputs at test-time with the intention of fooling it, thus causing the model to misclassify the input. In some domains, such as autonomous driving, the slightest perturbation to an input can have dire consequences in the real world if it results in a classification error(misclassifying a red light as a green light, for instance). These attacks can be haphazard and untargeted, where the pixel intensities of the input image are changed randomly in order to lower the confidence of the original class and get any other prediction, or they can be more targeted, modifying the input x slightly towards a specific target class y. One method of injecting adversarial examples into a NN is through the learning process. The model update equation that is utilized during the backpropogation process can be modified to hold the parameter theta and the class label y fixed, while only changing the input x, with the aim of maximizing the loss of the model(make it less confident with its correct prediction).\n",
    "\n",
    "One way of proactively defending against adversarial attacks is simply to train the model with adversarial examples. By augmenting the training dataset through the inclusion of adversarial examples, the model can become more robust and regularized. In other words, it will get trained to use more robust features to classify an input and ignore more fragile ones. That being said, the addition of more data also implies longer overhead time for training. Another way of protecting against adversarial attacks is called 'feature squeezing,' which is implemented as follows:\n",
    "\n",
    "1) calculate prediction on input image as normal\n",
    "2) reduce the color depth of the image(limit the number of colors each pixel can represent)\n",
    "3) perform spatial smoothing across the image to reduce large differences between pixels that are adjacent to one another\n",
    "4) calculate the L1 distance between the original image and both of the modified images. If the distance exceeds a certain threshold, it is marked as adversarial.\n",
    "\n",
    "The intuition behind feature squeezing is that most inputs contain many more features than are necessary in order for the network to be able to successfully classify an image. As a simplified example, it's easy to classify an object in an image as a bird simply by observing whether the object has feathers and a beak, without the need for further information. Thus, training the model to classify an image by looking for only a few essential features minimizes the avenues of attack for an adversarial assault(less features for it to modify and throw the model off).\n",
    "\n",
    "\n",
    "(Link to feature squeezing source https://towardsdatascience.com/fooling-neural-networks-with-adversarial-examples-8afd36258a03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdd75c5b4ec3903f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4: Retraining (20 points)\n",
    "\n",
    "A company has found that their internal image recognition tool is producing empirically poor output for a set of images submitted by their users. Upon further investigation, it is determined that these poor performing images are adversarial attacks designed to fool their image recognition software. However, the same set of images are reused by these attackers over and over. Given this set of successful adversarial attacks, how could this be mitigated in the short run? Will this work in the long run? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d3b35e93dbc5a77f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In this scenario, it is likely that an external party has figured out a way to execute a targeted adversarial attack on the company's model by submitting the same sets of adversarial images over and over again, likely with the aim of nudging the pixel space imperceptibly in order to make the model learn an incorrect classification probability for the images in question. One simple, brute force defense technique that could work in order to help the company mitigate this issue in the short run would be to retrain the model with many adversarial examples of those images and explicitly train it not to be fooled by them. Another potential strategy that could be used to combat this problem is defensive distillation. With distillation, a single model is trained initially that outputs the usual \"hard\" labels of a class(i.e. dog or cat). By itself, this model is easy to exploit, because the attacker can just figure out what features the model is scanning for to reach its classification decisions. However, a second model is also added as a form of filter, which only outputs \"soft\" labels for the input x(i.e. 95% chance it is a dog, as opposed to a simple \"yes\" or \"no\"). This randommness that is added to gain a perfect match makes it more difficult for an attacker to hijack the system by simply learning the training scheme of one hard classifier.\n",
    "\n",
    "These methods can potentially work for the short term, but should not be anticipated to serve as a permanent solution. This is because this solution is not an adaptive one. While it will help the network learn to recognize those specific adversarial images, with enough time and effort, the attacker could still choose a different set of images and nudge those images in a direction that the model has never seen before. Thus, while it is possible to take some pro-active measures to reduce the risk of adversarial attacks like this, it is likely that the company will continue to have to be \"reactive\" against new adversarial attacks their model has never seen.\n",
    "\n",
    "\n",
    "(defensive distillation source https://openai.com/blog/adversarial-example-research/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58eaae5215ab9ca0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5: Applications (20 points)\n",
    "\n",
    "What are potential applications of GANs? Research and briefly present three machine learning innovations that used GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-756d167bcd92b2c0",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Augment data by generating new examples: One application of GANs would be to augment a dataset by generating new plausible examples that could belong in the dataset. While it is possible to attempt to do this manually, such a process is very time consuming. The GAN is also able to detect structure in the data to make the \"fake\" data more realistic that the human eye can easily miss. Additionally, human judgement is quite subjective. A potential utilization of this would be in the domain of credit card fraud. Since fraudulent transactions don't occur nearly as often as legitimate transactions, it might be useful to utilize a GAN model to generate fake instances of fraudulent transactions, which can then be used to train a fraud detection model. \n",
    "\n",
    "Generate photorealistic images from semantic images: It is possible to utilize GANs to synthesize high quality, photorealistic images from low-level semantic imagery(otherwise known as translation). This can be useful within the domain of autonomous driving, since Radar/Lidar systems often capture low level imagery as opposed to photorealistic imagery, thereby providing the data scientist more images with which to train their in-car image detection algorithms. \n",
    "\n",
    "\n",
    "\n",
    "Text-to-image translation: Another relatively new domain of GAN application is text to image translation, where the GAN is able to synthesize realistic images based on a linguistic description. This can have far reaching applicability, in domains like graphic design, photo editing, and CAD, from an engineering standpoint. However, Vanilla GANs have been shown to perform poorly at this kind of task, as adding additional layers to upsample the input results in lots of instability and poor output. Therefore, for text-to-image tasks, two GANs are typically stacked together. The first GAN sketches the primitive shapes and colors from the text description, ultimately creating a low-resolution image. The second GAN then corrects for errors in the low-resolution image and fills in the details based on the text-description, resulting in a high-resolution image.\n",
    "\n",
    "\n",
    "(Augment Data source) https://www.toptal.com/machine-learning/generative-adversarial-networks\n",
    "(Image translation source)  https://arxiv.org/abs/1711.11585\n",
    "(text-to-image translation) https://blog.eduonix.com/artificial-intelligence/grand-finale-applications-gans/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
